{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd95df06ad6614d8ea31ce2c635a29972",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "from torch_geometric.datasets import CoraFull, TUDataset\n",
    "import numpy as np\n",
    "import torch_geometric as tg\n",
    "from utils import count_in_degree, count_out_degree, create_pointer_graph\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "source": [
    "## add dataset and shuffle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphs = TUDataset(\"\", \"PROTEINS\")\n",
    "# cora = CoraFull(\"\", \"Cora\").data\n",
    "perm = np.random.choice(list(range(len(graphs))), len(graphs), replace=False)\n",
    "graphs = [graphs[int(i)] for i in perm]\n",
    "ys = [graph.y.unsqueeze(0) for graph in graphs]\n",
    "ys = torch.cat(ys)"
   ]
  },
  {
   "source": [
    "## get train and test datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_test = ys[-100:]\n",
    "graphs_test = graphs[-100:]\n",
    "graphs_train = graphs[:-100]\n",
    "ys_train = ys[:-100]"
   ]
  },
  {
   "source": [
    "## create starting embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embdim = 64\n",
    "for graph in graphs:\n",
    "    zeros = torch.zeros(graph.x.shape[0], 16-graph.x.shape[1])\n",
    "    graph.x = torch.cat((graph.x, zeros), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, embdim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(embdim, embdim)\n",
    "        self.mlp = MLP([2*embdim, 64, embdim])\n",
    "    def forward(self, x, graph, igraph):\n",
    "        newx   = torch.zeros_like(x)\n",
    "        self_h = self.lin(x)\n",
    "\n",
    "        # uncomment to normalize \n",
    "        h = x\n",
    "        # h = x / degs[igraph]['out']\n",
    "\n",
    "        edges = graph.edge_index\n",
    "        sp = torch.sparse_coo_tensor(edges, torch.ones(len(edges[0])), [len(x), len(x)])\n",
    "        newx = torch.spmm(sp, h)\n",
    "        \n",
    "        newx = torch.cat((newx, self_h), dim=1)\n",
    "        newx = self.mlp(newx) + x\n",
    "        return newx\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, arch, last_activation=F.relu, middle_activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.lins = nn.ModuleList([nn.Linear(a, b) for a, b in zip(arch[:-1], arch[1:])])\n",
    "        self.last_activation = last_activation\n",
    "        self.middle_activation = middle_activation\n",
    "    def forward(self, h):\n",
    "        for lin in self.lins[:-1]:\n",
    "            h = self.middle_activation(lin(h))\n",
    "        h = self.last_activation(self.lins[-1](h))\n",
    "        return h\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n, embdim=16):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([GCNLayer(embdim) for i in range(n)])\n",
    "        self.classifier = MLP([embdim, 32, 2], last_activation= lambda x: torch.softmax(x, dim=-1))\n",
    "        self.embdim = embdim\n",
    "    def forward(self, graph, igraph):\n",
    "        h = graph.x.clone()\n",
    "        zeros = torch.zeros(h.shape[0], self.embdim-h.shape[1])\n",
    "        h = torch.cat((h, zeros), dim=1)     \n",
    "        for layer in self.layers:\n",
    "            h = layer(h, graph, igraph)\n",
    "        m = h.mean(dim=0, keepdim=True)\n",
    "        preds = self.classifier(m)\n",
    "        return preds, m\n",
    "\n",
    "model = Model(7)"
   ]
  },
  {
   "source": [
    "# Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.008)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 545 0.5674636363983154 0.65625\n",
      "17 546 0.5844385623931885 0.75\n",
      "17 547 0.6784171462059021 0.53125\n",
      "17 548 0.5347244739532471 0.78125\n",
      "17 549 0.6111361980438232 0.71875\n",
      "17 550 0.4236001670360565 0.84375\n",
      "17 551 0.5886922478675842 0.75\n",
      "17 552 0.40338483452796936 0.875\n",
      "17 553 0.6657277941703796 0.625\n",
      "17 554 0.48944056034088135 0.78125\n",
      "17 555 0.6872168779373169 0.6875\n",
      "17 556 0.6219539046287537 0.75\n",
      "17 557 0.5899772047996521 0.78125\n",
      "17 558 0.5678197741508484 0.65625\n",
      "17 559 0.497464120388031 0.75\n",
      "17 560 0.5440641641616821 0.6875\n",
      "17 561 0.5503969788551331 0.75\n",
      "17 562 0.5817108154296875 0.625\n",
      "17 563 0.5466321706771851 0.75\n",
      "17 564 0.5236157178878784 0.75\n",
      "17 565 0.4851984977722168 0.78125\n",
      "17 566 0.529097318649292 0.6875\n",
      "17 567 0.5717544555664062 0.75\n",
      "17 568 0.500135064125061 0.78125\n",
      "17 569 0.6102918386459351 0.6875\n",
      "18 570 0.504416286945343 0.75\n",
      "18 571 0.705080509185791 0.59375\n",
      "18 572 0.6129415035247803 0.6875\n",
      "18 573 0.4417411684989929 0.84375\n",
      "18 574 0.7552437782287598 0.59375\n",
      "18 575 0.44240716099739075 0.84375\n",
      "18 576 0.6095021367073059 0.59375\n",
      "18 577 0.6176207065582275 0.59375\n",
      "18 578 0.5501571297645569 0.75\n",
      "18 579 0.6164032816886902 0.625\n",
      "18 580 0.6637954711914062 0.625\n",
      "18 581 0.5018805861473083 0.84375\n",
      "18 582 0.42822471261024475 0.84375\n",
      "18 583 0.5148150324821472 0.84375\n",
      "18 584 0.424550324678421 0.875\n",
      "18 585 0.6811743974685669 0.59375\n",
      "18 586 0.5467308163642883 0.75\n",
      "18 587 0.6136608123779297 0.71875\n",
      "18 588 0.6795765161514282 0.71875\n",
      "18 589 0.5422459244728088 0.75\n",
      "18 590 0.5733239650726318 0.6875\n",
      "18 591 0.47758519649505615 0.75\n",
      "18 592 0.5568183660507202 0.71875\n",
      "18 593 0.5160502195358276 0.78125\n",
      "18 594 0.6209243535995483 0.5625\n",
      "18 595 0.5185019373893738 0.8125\n",
      "18 596 0.5470103025436401 0.6875\n",
      "18 597 0.49931600689888 0.78125\n",
      "18 598 0.468281090259552 0.71875\n",
      "18 599 0.5983496904373169 0.71875\n",
      "18 600 0.5368914604187012 0.78125\n",
      "18 601 0.5801555514335632 0.6875\n",
      "19 602 0.5434967875480652 0.75\n",
      "19 603 0.6548588275909424 0.65625\n",
      "19 604 0.5165701508522034 0.71875\n",
      "19 605 0.4996640086174011 0.8125\n",
      "19 606 0.6744588017463684 0.6875\n",
      "19 607 0.5375685691833496 0.6875\n",
      "19 608 0.5965853333473206 0.71875\n",
      "19 609 0.6050463914871216 0.625\n",
      "19 610 0.6757084131240845 0.625\n",
      "19 611 0.4696322977542877 0.78125\n",
      "19 612 0.7097151279449463 0.65625\n",
      "19 613 0.43310168385505676 0.8125\n",
      "19 614 0.4570661783218384 0.84375\n",
      "19 615 0.567960262298584 0.6875\n",
      "19 616 0.5456367135047913 0.6875\n",
      "19 617 0.5706697106361389 0.71875\n",
      "19 618 0.5242825746536255 0.78125\n",
      "19 619 0.648966908454895 0.6875\n",
      "19 620 0.5494787096977234 0.8125\n",
      "19 621 0.5616896152496338 0.6875\n",
      "19 622 0.5387909412384033 0.71875\n",
      "19 623 0.5353131294250488 0.71875\n",
      "19 624 0.5368310213088989 0.75\n",
      "19 625 0.5782231092453003 0.71875\n",
      "19 626 0.5297890305519104 0.71875\n",
      "19 627 0.45857590436935425 0.8125\n",
      "19 628 0.5416364073753357 0.71875\n",
      "19 629 0.5130482316017151 0.65625\n",
      "19 630 0.4860175848007202 0.75\n",
      "19 631 0.6429571509361267 0.71875\n",
      "19 632 0.5527639389038086 0.65625\n",
      "19 633 0.5224595069885254 0.75\n",
      "20 634 0.6857304573059082 0.65625\n",
      "20 635 0.5412225723266602 0.65625\n",
      "20 636 0.4434862732887268 0.875\n",
      "20 637 0.6372466087341309 0.71875\n",
      "20 638 0.5215663313865662 0.78125\n",
      "20 639 0.6013858914375305 0.625\n",
      "20 640 0.5831741094589233 0.65625\n",
      "20 641 0.5870659947395325 0.75\n",
      "20 642 0.7279677987098694 0.5\n",
      "20 643 0.4858599007129669 0.78125\n",
      "20 644 0.6503981351852417 0.71875\n",
      "20 645 0.40865057706832886 0.84375\n",
      "20 646 0.5417211651802063 0.8125\n",
      "20 647 0.40509867668151855 0.875\n",
      "20 648 0.7021761536598206 0.5625\n",
      "20 649 0.5079956650733948 0.78125\n",
      "20 650 0.5829404592514038 0.75\n",
      "20 651 0.5895385146141052 0.75\n",
      "20 652 0.627126932144165 0.75\n",
      "20 653 0.525850772857666 0.6875\n",
      "20 654 0.4953238368034363 0.75\n",
      "20 655 0.5328333973884583 0.75\n",
      "20 656 0.5520473122596741 0.71875\n",
      "20 657 0.5767097473144531 0.65625\n",
      "20 658 0.5239487886428833 0.78125\n",
      "20 659 0.5456273555755615 0.6875\n",
      "20 660 0.46343332529067993 0.8125\n",
      "20 661 0.5045585036277771 0.65625\n",
      "20 662 0.5557366609573364 0.75\n",
      "20 663 0.4948175847530365 0.78125\n",
      "20 664 0.6208139061927795 0.6875\n",
      "21 665 0.5567989349365234 0.71875\n",
      "21 666 0.6197978258132935 0.71875\n",
      "21 667 0.643543541431427 0.59375\n",
      "21 668 0.4177178740501404 0.875\n",
      "21 669 0.7804690003395081 0.59375\n",
      "21 670 0.4128967523574829 0.84375\n",
      "21 671 0.5981912612915039 0.6875\n",
      "21 672 0.662617564201355 0.5625\n",
      "21 673 0.5600336194038391 0.75\n",
      "21 674 0.590049147605896 0.65625\n",
      "21 675 0.6682706475257874 0.625\n",
      "21 676 0.5105510950088501 0.8125\n",
      "21 677 0.43587759137153625 0.84375\n",
      "21 678 0.4572857618331909 0.84375\n",
      "21 679 0.45884278416633606 0.84375\n",
      "21 680 0.667262077331543 0.625\n",
      "21 681 0.5609259605407715 0.71875\n",
      "21 682 0.5747268199920654 0.78125\n",
      "21 683 0.6965395212173462 0.6875\n",
      "21 684 0.5314146280288696 0.78125\n",
      "21 685 0.5567882657051086 0.6875\n",
      "21 686 0.5064678192138672 0.71875\n",
      "21 687 0.5371252298355103 0.75\n",
      "21 688 0.5010262131690979 0.78125\n",
      "21 689 0.6721364259719849 0.53125\n",
      "21 690 0.47298169136047363 0.84375\n",
      "21 691 0.5480871200561523 0.6875\n",
      "21 692 0.5261884331703186 0.78125\n",
      "21 693 0.4363210201263428 0.75\n",
      "21 694 0.6009167432785034 0.75\n",
      "21 695 0.5638748407363892 0.75\n",
      "21 696 0.5513345003128052 0.71875\n",
      "22 697 0.5554751753807068 0.71875\n",
      "22 698 0.636898934841156 0.65625\n",
      "22 699 0.5283703804016113 0.71875\n",
      "22 700 0.5059272050857544 0.84375\n",
      "22 701 0.6448035836219788 0.71875\n",
      "22 702 0.5486301779747009 0.65625\n",
      "22 703 0.5588988065719604 0.71875\n",
      "22 704 0.6109446287155151 0.625\n",
      "22 705 0.6774697303771973 0.59375\n",
      "22 706 0.4603164792060852 0.78125\n",
      "22 707 0.6986755728721619 0.65625\n",
      "22 708 0.42632371187210083 0.84375\n",
      "22 709 0.4800036549568176 0.8125\n",
      "22 710 0.528786838054657 0.75\n",
      "22 711 0.5527605414390564 0.6875\n",
      "22 712 0.548346996307373 0.78125\n",
      "22 713 0.558017909526825 0.75\n",
      "22 714 0.6187864542007446 0.71875\n",
      "22 715 0.5528784394264221 0.8125\n",
      "22 716 0.5770981311798096 0.6875\n",
      "22 717 0.5374730825424194 0.75\n",
      "22 718 0.5378335118293762 0.6875\n",
      "22 719 0.5291330218315125 0.75\n",
      "22 720 0.5838965177536011 0.71875\n",
      "22 721 0.5167925357818604 0.71875\n",
      "22 722 0.4881352186203003 0.78125\n",
      "22 723 0.48351165652275085 0.78125\n",
      "22 724 0.5288798809051514 0.65625\n",
      "22 725 0.4963884949684143 0.75\n",
      "22 726 0.6311893463134766 0.71875\n",
      "22 727 0.5712254047393799 0.6875\n",
      "22 728 0.5171316266059875 0.71875\n",
      "23 729 0.6842820644378662 0.6875\n",
      "23 730 0.529842734336853 0.65625\n",
      "23 731 0.4715127944946289 0.84375\n",
      "23 732 0.6936497688293457 0.6875\n",
      "23 733 0.48175424337387085 0.8125\n",
      "23 734 0.5839850306510925 0.65625\n",
      "23 735 0.6613208055496216 0.625\n",
      "23 736 0.532748281955719 0.71875\n",
      "23 737 0.6896660327911377 0.5\n",
      "23 738 0.5007576942443848 0.78125\n",
      "23 739 0.6181654930114746 0.71875\n",
      "23 740 0.39903029799461365 0.875\n",
      "23 741 0.5472954511642456 0.75\n",
      "23 742 0.38639160990715027 0.875\n",
      "23 743 0.7052690982818604 0.625\n",
      "23 744 0.4935067892074585 0.75\n",
      "23 745 0.5570264458656311 0.75\n",
      "23 746 0.5892837047576904 0.78125\n",
      "23 747 0.6437565684318542 0.65625\n",
      "23 748 0.5012689232826233 0.71875\n",
      "23 749 0.4938911199569702 0.75\n",
      "23 750 0.5241114497184753 0.75\n",
      "23 751 0.553381085395813 0.75\n",
      "23 752 0.5746495723724365 0.65625\n",
      "23 753 0.5244707465171814 0.78125\n",
      "23 754 0.5370157957077026 0.6875\n",
      "23 755 0.48882144689559937 0.78125\n",
      "23 756 0.48561742901802063 0.625\n",
      "23 757 0.5457330346107483 0.75\n",
      "23 758 0.5144774913787842 0.78125\n",
      "23 759 0.6134876608848572 0.65625\n",
      "24 760 0.5666124820709229 0.6875\n",
      "24 761 0.5993573069572449 0.75\n",
      "24 762 0.619827389717102 0.59375\n",
      "24 763 0.4777447581291199 0.84375\n",
      "24 764 0.7361851930618286 0.65625\n",
      "24 765 0.4296550750732422 0.78125\n",
      "24 766 0.6093324422836304 0.65625\n",
      "24 767 0.6612334251403809 0.59375\n",
      "24 768 0.5362811088562012 0.78125\n",
      "24 769 0.5556002855300903 0.65625\n",
      "24 770 0.6995818614959717 0.625\n",
      "24 771 0.45200395584106445 0.875\n",
      "24 772 0.4368079900741577 0.84375\n",
      "24 773 0.4949392080307007 0.8125\n",
      "24 774 0.4750003516674042 0.78125\n",
      "24 775 0.6486440896987915 0.625\n",
      "24 776 0.5601069927215576 0.71875\n",
      "24 777 0.5327984094619751 0.8125\n",
      "24 778 0.6174179315567017 0.71875\n",
      "24 779 0.5517861247062683 0.71875\n",
      "24 780 0.5566431283950806 0.71875\n",
      "24 781 0.4874163269996643 0.71875\n",
      "24 782 0.5459215641021729 0.78125\n",
      "24 783 0.5350357890129089 0.71875\n",
      "24 784 0.6100616455078125 0.59375\n",
      "24 785 0.48480141162872314 0.8125\n",
      "24 786 0.5090587735176086 0.71875\n",
      "24 787 0.5296888947486877 0.6875\n",
      "24 788 0.4275903105735779 0.78125\n",
      "24 789 0.6144932508468628 0.71875\n",
      "24 790 0.564075767993927 0.71875\n",
      "24 791 0.5363799333572388 0.75\n",
      "25 792 0.587989091873169 0.75\n",
      "25 793 0.599623441696167 0.71875\n",
      "25 794 0.5106531977653503 0.71875\n",
      "25 795 0.549443244934082 0.78125\n",
      "25 796 0.6124498248100281 0.75\n",
      "25 797 0.552902102470398 0.65625\n",
      "25 798 0.5451967120170593 0.71875\n",
      "25 799 0.6083588004112244 0.625\n",
      "25 800 0.6695339679718018 0.5625\n",
      "25 801 0.4887622594833374 0.75\n",
      "25 802 0.6918863654136658 0.65625\n",
      "25 803 0.39780932664871216 0.875\n",
      "25 804 0.4758211672306061 0.8125\n",
      "25 805 0.5184717178344727 0.71875\n",
      "25 806 0.5556342005729675 0.6875\n",
      "25 807 0.5649042129516602 0.71875\n",
      "25 808 0.5611281394958496 0.75\n",
      "25 809 0.5871458053588867 0.75\n",
      "25 810 0.5464428067207336 0.8125\n",
      "25 811 0.5609390139579773 0.6875\n",
      "25 812 0.5543481707572937 0.71875\n",
      "25 813 0.5194100141525269 0.71875\n",
      "25 814 0.5246559381484985 0.71875\n",
      "25 815 0.5969319343566895 0.6875\n",
      "25 816 0.48613250255584717 0.75\n",
      "25 817 0.4857524633407593 0.78125\n",
      "25 818 0.4736552834510803 0.8125\n",
      "25 819 0.5381254553794861 0.625\n",
      "25 820 0.5045758485794067 0.75\n",
      "25 821 0.6024225950241089 0.75\n",
      "25 822 0.5880849361419678 0.65625\n",
      "25 823 0.484594464302063 0.78125\n",
      "26 824 0.6736195087432861 0.75\n",
      "26 825 0.5382289886474609 0.65625\n",
      "26 826 0.5132067203521729 0.8125\n",
      "26 827 0.624230146408081 0.71875\n",
      "26 828 0.46789389848709106 0.8125\n",
      "26 829 0.5871056318283081 0.625\n",
      "26 830 0.6256154179573059 0.65625\n",
      "26 831 0.5637712478637695 0.71875\n",
      "26 832 0.6901255249977112 0.46875\n",
      "26 833 0.5308538675308228 0.75\n",
      "26 834 0.5813202261924744 0.75\n",
      "26 835 0.39513587951660156 0.84375\n",
      "26 836 0.5393685102462769 0.75\n",
      "26 837 0.40130677819252014 0.84375\n",
      "26 838 0.69976806640625 0.65625\n",
      "26 839 0.4801103472709656 0.78125\n",
      "26 840 0.5711387991905212 0.75\n",
      "26 841 0.65920490026474 0.75\n",
      "26 842 0.5698485970497131 0.78125\n",
      "26 843 0.4947361946105957 0.71875\n",
      "26 844 0.4960564374923706 0.75\n",
      "26 845 0.5202321410179138 0.75\n",
      "26 846 0.5483077168464661 0.71875\n",
      "26 847 0.6149470210075378 0.625\n",
      "26 848 0.4706607758998871 0.8125\n",
      "26 849 0.5360332727432251 0.6875\n",
      "26 850 0.4943418800830841 0.71875\n",
      "26 851 0.4853968918323517 0.625\n",
      "26 852 0.5511344075202942 0.78125\n",
      "26 853 0.5090804100036621 0.8125\n",
      "26 854 0.5914350152015686 0.6875\n",
      "27 855 0.5779454112052917 0.6875\n",
      "27 856 0.6618533134460449 0.71875\n",
      "27 857 0.5835855007171631 0.65625\n",
      "27 858 0.4741749167442322 0.84375\n",
      "27 859 0.7043145895004272 0.65625\n",
      "27 860 0.4544207453727722 0.8125\n",
      "27 861 0.5680198669433594 0.6875\n",
      "27 862 0.6371583938598633 0.625\n",
      "27 863 0.5620589256286621 0.75\n",
      "27 864 0.542432963848114 0.65625\n",
      "27 865 0.6935608983039856 0.59375\n",
      "27 866 0.46522605419158936 0.84375\n",
      "27 867 0.4346817135810852 0.84375\n",
      "27 868 0.4956824481487274 0.78125\n",
      "27 869 0.49820393323898315 0.71875\n",
      "27 870 0.6701980829238892 0.59375\n",
      "27 871 0.5345030426979065 0.75\n",
      "27 872 0.5127869844436646 0.8125\n",
      "27 873 0.6348971128463745 0.6875\n",
      "27 874 0.574059247970581 0.71875\n",
      "27 875 0.5288720726966858 0.78125\n",
      "27 876 0.5005554556846619 0.75\n",
      "27 877 0.5194989442825317 0.8125\n",
      "27 878 0.5783861875534058 0.6875\n",
      "27 879 0.6049858331680298 0.59375\n",
      "27 880 0.45817238092422485 0.84375\n",
      "27 881 0.4946472942829132 0.75\n",
      "27 882 0.5508450269699097 0.6875\n",
      "27 883 0.4410404562950134 0.75\n",
      "27 884 0.6075787544250488 0.71875\n",
      "27 885 0.5574846863746643 0.71875\n",
      "27 886 0.5212454795837402 0.71875\n",
      "28 887 0.5889647006988525 0.71875\n",
      "28 888 0.5995272994041443 0.71875\n",
      "28 889 0.5226021409034729 0.71875\n",
      "28 890 0.5459377765655518 0.8125\n",
      "28 891 0.6090800166130066 0.71875\n",
      "28 892 0.5444179177284241 0.71875\n",
      "28 893 0.5589690208435059 0.6875\n",
      "28 894 0.5765991806983948 0.6875\n",
      "28 895 0.7194163203239441 0.53125\n",
      "28 896 0.5221959352493286 0.75\n",
      "28 897 0.6286317110061646 0.71875\n",
      "28 898 0.4432286024093628 0.8125\n",
      "28 899 0.4208284020423889 0.875\n",
      "28 900 0.4771421551704407 0.8125\n",
      "28 901 0.527007520198822 0.78125\n",
      "28 902 0.5592386722564697 0.71875\n",
      "28 903 0.6722012758255005 0.625\n",
      "28 904 0.5806993246078491 0.78125\n",
      "28 905 0.5265536904335022 0.84375\n",
      "28 906 0.5861325263977051 0.65625\n",
      "28 907 0.5351746082305908 0.78125\n",
      "28 908 0.5558170676231384 0.65625\n",
      "28 909 0.5233758687973022 0.78125\n",
      "28 910 0.5809466242790222 0.65625\n",
      "28 911 0.5370065569877625 0.75\n",
      "28 912 0.433224081993103 0.84375\n",
      "28 913 0.4678879976272583 0.78125\n",
      "28 914 0.5239944458007812 0.625\n",
      "28 915 0.49783483147621155 0.78125\n",
      "28 916 0.6321958303451538 0.78125\n",
      "28 917 0.5805363655090332 0.6875\n",
      "28 918 0.488989919424057 0.8125\n",
      "29 919 0.6734893321990967 0.6875\n",
      "29 920 0.5917346477508545 0.65625\n",
      "29 921 0.4820254445075989 0.8125\n",
      "29 922 0.641859769821167 0.71875\n",
      "29 923 0.48028481006622314 0.78125\n",
      "29 924 0.5505792498588562 0.6875\n",
      "29 925 0.6269863843917847 0.625\n",
      "29 926 0.5518177151679993 0.71875\n",
      "29 927 0.6786505579948425 0.46875\n",
      "29 928 0.6257289052009583 0.71875\n",
      "29 929 0.5742632150650024 0.78125\n",
      "29 930 0.4170367419719696 0.90625\n",
      "29 931 0.5632757544517517 0.78125\n",
      "29 932 0.42056065797805786 0.8125\n",
      "29 933 0.6912761330604553 0.625\n",
      "29 934 0.477195680141449 0.78125\n",
      "29 935 0.6019060611724854 0.71875\n",
      "29 936 0.609650731086731 0.78125\n",
      "29 937 0.5608084201812744 0.78125\n",
      "29 938 0.489780068397522 0.71875\n",
      "29 939 0.48580992221832275 0.78125\n",
      "29 940 0.561431884765625 0.6875\n",
      "29 941 0.5325199365615845 0.78125\n",
      "29 942 0.6169402599334717 0.65625\n",
      "29 943 0.4654574394226074 0.8125\n",
      "29 944 0.5316373109817505 0.6875\n",
      "29 945 0.5011379718780518 0.71875\n",
      "29 946 0.46140870451927185 0.65625\n",
      "29 947 0.5671994686126709 0.78125\n",
      "29 948 0.49322444200515747 0.75\n",
      "29 949 0.6141369342803955 0.6875\n",
      "30 950 0.5767657160758972 0.6875\n",
      "30 951 0.717299222946167 0.71875\n",
      "30 952 0.5709721446037292 0.71875\n",
      "30 953 0.464456707239151 0.84375\n",
      "30 954 0.7319698929786682 0.6875\n",
      "30 955 0.4694589376449585 0.75\n",
      "30 956 0.6099128127098083 0.65625\n",
      "30 957 0.6176280975341797 0.6875\n",
      "30 958 0.5695396065711975 0.71875\n",
      "30 959 0.5433263182640076 0.75\n",
      "30 960 0.7060866355895996 0.59375\n",
      "30 961 0.44412195682525635 0.875\n",
      "30 962 0.4414685368537903 0.875\n",
      "30 963 0.49222439527511597 0.78125\n",
      "30 964 0.4818460941314697 0.78125\n",
      "30 965 0.6686453819274902 0.625\n",
      "30 966 0.5394201874732971 0.75\n",
      "30 967 0.5471852421760559 0.78125\n",
      "30 968 0.5969042778015137 0.75\n",
      "30 969 0.5425060987472534 0.71875\n",
      "30 970 0.5570194125175476 0.71875\n",
      "30 971 0.5005043745040894 0.71875\n",
      "30 972 0.4917325973510742 0.8125\n",
      "30 973 0.5822215676307678 0.6875\n",
      "30 974 0.6390136480331421 0.59375\n",
      "30 975 0.46254658699035645 0.8125\n",
      "30 976 0.5434026122093201 0.71875\n",
      "30 977 0.5307183265686035 0.71875\n",
      "30 978 0.44642701745033264 0.75\n",
      "30 979 0.6058080792427063 0.75\n",
      "30 980 0.5746901631355286 0.6875\n",
      "30 981 0.5142664909362793 0.78125\n",
      "31 982 0.5991479754447937 0.6875\n",
      "31 983 0.6089054346084595 0.71875\n",
      "31 984 0.5130721926689148 0.71875\n",
      "31 985 0.5829378366470337 0.75\n",
      "31 986 0.572848379611969 0.75\n",
      "31 987 0.5668156147003174 0.65625\n",
      "31 988 0.5573724508285522 0.6875\n",
      "31 989 0.5732702612876892 0.71875\n",
      "31 990 0.6693339347839355 0.5625\n",
      "31 991 0.5651346445083618 0.75\n",
      "31 992 0.6464194059371948 0.6875\n",
      "31 993 0.4159412384033203 0.875\n",
      "31 994 0.47172701358795166 0.84375\n",
      "31 995 0.4589559733867645 0.78125\n",
      "31 996 0.5835953950881958 0.71875\n",
      "31 997 0.5023696422576904 0.78125\n",
      "31 998 0.6848829388618469 0.625\n",
      "31 999 0.575325071811676 0.78125\n",
      "31 1000 0.5724970698356628 0.8125\n",
      "31 1001 0.562643826007843 0.6875\n",
      "31 1002 0.5468754768371582 0.71875\n",
      "31 1003 0.5370800495147705 0.65625\n",
      "31 1004 0.5386120080947876 0.8125\n",
      "31 1005 0.5758711695671082 0.65625\n",
      "31 1006 0.5859407186508179 0.6875\n",
      "31 1007 0.45805731415748596 0.8125\n",
      "31 1008 0.507220447063446 0.71875\n",
      "31 1009 0.5220393538475037 0.71875\n",
      "31 1010 0.4740564227104187 0.78125\n",
      "31 1011 0.6181330680847168 0.75\n",
      "31 1012 0.5773371458053589 0.6875\n",
      "31 1013 0.49068158864974976 0.84375\n",
      "32 1014 0.6837580800056458 0.6875\n",
      "32 1015 0.670133113861084 0.59375\n",
      "32 1016 0.49264001846313477 0.8125\n",
      "32 1017 0.716810405254364 0.71875\n",
      "32 1018 0.46594101190567017 0.78125\n",
      "32 1019 0.5347981452941895 0.65625\n",
      "32 1020 0.6796493530273438 0.59375\n",
      "32 1021 0.5518208146095276 0.71875\n",
      "32 1022 0.6452819108963013 0.5625\n",
      "32 1023 0.6178043484687805 0.6875\n",
      "32 1024 0.5221418142318726 0.8125\n",
      "32 1025 0.40966475009918213 0.9375\n",
      "32 1026 0.5684288144111633 0.75\n",
      "32 1027 0.42283201217651367 0.875\n",
      "32 1028 0.6916142702102661 0.59375\n",
      "32 1029 0.49009501934051514 0.78125\n",
      "32 1030 0.5788779258728027 0.71875\n",
      "32 1031 0.5878139734268188 0.78125\n",
      "32 1032 0.5735023021697998 0.75\n",
      "32 1033 0.5384904742240906 0.71875\n",
      "32 1034 0.4402976632118225 0.78125\n",
      "32 1035 0.5266770720481873 0.6875\n",
      "32 1036 0.5545262098312378 0.75\n",
      "32 1037 0.6571387052536011 0.625\n",
      "32 1038 0.43535298109054565 0.84375\n",
      "32 1039 0.5424103736877441 0.6875\n",
      "32 1040 0.523879885673523 0.71875\n",
      "32 1041 0.4288609027862549 0.71875\n",
      "32 1042 0.565316915512085 0.8125\n",
      "32 1043 0.4995654821395874 0.78125\n",
      "32 1044 0.6068419218063354 0.65625\n",
      "33 1045 0.573515772819519 0.71875\n",
      "33 1046 0.6746677756309509 0.71875\n",
      "33 1047 0.6022440791130066 0.65625\n",
      "33 1048 0.4303690791130066 0.875\n",
      "33 1049 0.7070434093475342 0.65625\n",
      "33 1050 0.46369779109954834 0.78125\n",
      "33 1051 0.597156286239624 0.65625\n",
      "33 1052 0.6191548109054565 0.65625\n",
      "33 1053 0.5745645761489868 0.71875\n",
      "33 1054 0.5179243683815002 0.71875\n",
      "33 1055 0.6840097904205322 0.625\n",
      "33 1056 0.4335446357727051 0.875\n",
      "33 1057 0.4409303069114685 0.875\n",
      "33 1058 0.48450496792793274 0.78125\n",
      "33 1059 0.5541345477104187 0.71875\n",
      "33 1060 0.6144192218780518 0.65625\n",
      "33 1061 0.5199470520019531 0.75\n",
      "33 1062 0.520027756690979 0.78125\n",
      "33 1063 0.6014266014099121 0.78125\n",
      "33 1064 0.5288640856742859 0.71875\n",
      "33 1065 0.5503326058387756 0.71875\n",
      "33 1066 0.5395359992980957 0.6875\n",
      "33 1067 0.42289406061172485 0.84375\n",
      "33 1068 0.6128733158111572 0.6875\n",
      "33 1069 0.5860595107078552 0.65625\n",
      "33 1070 0.4646223783493042 0.8125\n",
      "33 1071 0.49035385251045227 0.75\n",
      "33 1072 0.5569066405296326 0.6875\n",
      "33 1073 0.4519369602203369 0.75\n",
      "33 1074 0.6031861901283264 0.65625\n",
      "33 1075 0.6035082936286926 0.6875\n",
      "33 1076 0.5080713033676147 0.6875\n",
      "34 1077 0.6251789331436157 0.6875\n",
      "34 1078 0.633014440536499 0.625\n",
      "34 1079 0.5225592851638794 0.75\n",
      "34 1080 0.5859875082969666 0.75\n",
      "34 1081 0.5977842211723328 0.75\n",
      "34 1082 0.5942672491073608 0.75\n",
      "34 1083 0.563591480255127 0.6875\n",
      "34 1084 0.6657918691635132 0.625\n",
      "34 1085 0.5995484590530396 0.625\n",
      "34 1086 0.593450129032135 0.71875\n",
      "34 1087 0.5981557369232178 0.6875\n",
      "34 1088 0.4587549865245819 0.8125\n",
      "34 1089 0.4773203134536743 0.78125\n",
      "34 1090 0.43885913491249084 0.84375\n",
      "34 1091 0.5649816393852234 0.71875\n",
      "34 1092 0.48170915246009827 0.78125\n",
      "34 1093 0.6545833945274353 0.71875\n",
      "34 1094 0.5341745018959045 0.8125\n",
      "34 1095 0.5779647827148438 0.78125\n",
      "34 1096 0.5870810151100159 0.65625\n",
      "34 1097 0.4950711131095886 0.75\n",
      "34 1098 0.5535346269607544 0.6875\n",
      "34 1099 0.5751030445098877 0.71875\n",
      "34 1100 0.5614413022994995 0.65625\n",
      "34 1101 0.6657305955886841 0.65625\n",
      "34 1102 0.4754796624183655 0.75\n",
      "34 1103 0.4778938293457031 0.78125\n",
      "34 1104 0.5999056100845337 0.6875\n",
      "34 1105 0.48629096150398254 0.78125\n",
      "34 1106 0.5846009254455566 0.75\n",
      "34 1107 0.6238067150115967 0.65625\n",
      "35 1108 0.46833908557891846 0.8125\n",
      "35 1109 0.7112072706222534 0.625\n",
      "35 1110 0.6187282800674438 0.625\n",
      "35 1111 0.5243037939071655 0.78125\n",
      "35 1112 0.7242742776870728 0.6875\n",
      "35 1113 0.4574938416481018 0.84375\n",
      "35 1114 0.5418763160705566 0.75\n",
      "35 1115 0.7148247361183167 0.5625\n",
      "35 1116 0.5293208956718445 0.78125\n",
      "35 1117 0.6127430200576782 0.65625\n",
      "35 1118 0.6223341226577759 0.71875\n",
      "35 1119 0.48459625244140625 0.78125\n",
      "35 1120 0.40180253982543945 0.9375\n",
      "35 1121 0.5281395316123962 0.75\n",
      "35 1122 0.4504934549331665 0.84375\n",
      "35 1123 0.6549145579338074 0.625\n",
      "35 1124 0.513124942779541 0.75\n",
      "35 1125 0.6231692433357239 0.6875\n",
      "35 1126 0.5959782004356384 0.78125\n",
      "35 1127 0.6261396408081055 0.75\n",
      "35 1128 0.5238114595413208 0.75\n",
      "35 1129 0.4952241778373718 0.75\n",
      "35 1130 0.5916273593902588 0.625\n",
      "35 1131 0.550176203250885 0.78125\n",
      "35 1132 0.6492304801940918 0.53125\n",
      "35 1133 0.4890560507774353 0.78125\n",
      "35 1134 0.5613433122634888 0.6875\n",
      "35 1135 0.4874914884567261 0.8125\n",
      "35 1136 0.42602917551994324 0.8125\n",
      "35 1137 0.6614912748336792 0.5\n",
      "35 1138 0.4456910789012909 0.78125\n",
      "35 1139 0.5694290399551392 0.71875\n",
      "36 1140 0.5799484252929688 0.6875\n",
      "36 1141 0.6592549085617065 0.71875\n",
      "36 1142 0.593761146068573 0.65625\n",
      "36 1143 0.4618918299674988 0.84375\n",
      "36 1144 0.6844984889030457 0.6875\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-ed2d998ea324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "! mkdir tb\n",
    "writer = SummaryWriter('tb/gcn_'+str(random.random())) \n",
    "eps = 100\n",
    "\n",
    "out_dim = 2\n",
    "bpreds = torch.zeros(0, out_dim)\n",
    "bsize, bpass, ibatch = 32, 0, 0\n",
    "ys = []\n",
    "for ep in range(0, eps):\n",
    "    for igraph, (graph, one_y) in enumerate(zip(graphs_train, ys_train)):\n",
    "        \n",
    "        preds, x = model(graph, igraph)\n",
    "        bpreds = torch.cat((bpreds, preds))\n",
    "        ys.append(one_y)\n",
    "        bpass += 1\n",
    "        if bpass < bsize:\n",
    "            continue\n",
    "\n",
    "        y = torch.tensor(ys)\n",
    "        yhot = F.one_hot(y.long(), num_classes=out_dim)\n",
    "        # print(y.shape, yhot.shape, bpreds.shape)\n",
    "        loss = -(yhot*torch.log(bpreds+1e-8) + (1-yhot)*torch.log(1-bpreds+1e-8)).mean()\n",
    "        acc = (((bpreds>0.5)==yhot).float().sum(dim=1)==yhot.shape[-1]).float().mean()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        ibatch += 1\n",
    "        bpass = 0\n",
    "        bpreds = torch.zeros(0, out_dim)\n",
    "        ys = []\n",
    "        \n",
    "        writer.add_scalar(\"loss\", loss.item(), ep*eps//bsize+ibatch)\n",
    "        writer.add_scalar(\"acc\" , acc.item() , ep*eps//bsize+ibatch)\n",
    "        print(ep, ibatch, loss.item(), acc.item())\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Try on validatation set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.6900)"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "bpreds = torch.zeros(0, out_dim)\n",
    "ys = []\n",
    "for igraph, (graph, one_y) in enumerate(zip(graphs_test, ys_test)):\n",
    "    \n",
    "    preds, x = model(graph, len(graphs_train)+igraph)\n",
    "    bpreds = torch.cat((bpreds, preds))\n",
    "    ys.append(one_y)\n",
    "\n",
    "y = torch.tensor(ys)\n",
    "yhot = F.one_hot(y.long(), num_classes=out_dim)\n",
    "# print(yhot.shape, bpreds.shape)\n",
    "loss = -(yhot*torch.log(bpreds+1e-8) + (1-yhot)*torch.log(1-bpreds+1e-8)).mean()\n",
    "acc = (((bpreds>0.5)==yhot).float().sum(dim=1)==yhot.shape[-1]).float().mean()\n",
    "acc"
   ]
  }
 ]
}